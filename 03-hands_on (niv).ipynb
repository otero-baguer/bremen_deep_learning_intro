{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "romsoc_hands_on_3_handout",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MuKBrPdbB0b",
        "colab_type": "text"
      },
      "source": [
        "# Hands-On Session No. 3\n",
        "## (Training CNNs on MNIST/CIFAR10)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JZ0f12TXc3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# this should print all folders in your google drive main folder\n",
        "!ls \"/content/gdrive/My Drive/\"\n",
        "\n",
        "# change \"your_data_folder\" to your data_dir name\n",
        "data_dir = \"/content/gdrive/My Drive/colab_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZxgJ3YVffj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "def mnist_loaders(batch_size, data_dir):\n",
        "  transform = transforms.ToTensor()\n",
        "  mnist_train = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "  mnist_test = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "  test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "\n",
        "def cifar_loaders(batch_size, data_dir):\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.225, 0.225, 0.225])\n",
        "  train = datasets.CIFAR10(data_dir, train=True, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.RandomHorizontalFlip(),\n",
        "                               transforms.RandomCrop(32, 4),\n",
        "                               transforms.ToTensor(),\n",
        "                               normalize,\n",
        "                           ]))\n",
        "  test = datasets.CIFAR10(data_dir, train=False, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "  train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "class AverageValueMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0.0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYDz_gizJtk3",
        "colab_type": "text"
      },
      "source": [
        "# Instructions for excercise\n",
        "\n",
        "### Q1\n",
        "\n",
        "1a. Implement a linear model.\n",
        "\n",
        "1b. Think, what do you think will happen if weights (and bias..) are initialized to zero? Now run it and check.\n",
        "\n",
        "1c. Did the model managed to learn? Why?\n",
        "\n",
        "1d. Train your model end to end on MNIST\n",
        "(you might want to check out momentum method)\n",
        "\n",
        "**GPU**:\n",
        "\n",
        "Try to move to computing on GPU. For that you'll need to \"move\" your model and all the data to be in the GPU. You can do it uding \".cuda()\" at the end of your model and all input tensors (e.g., samples and targets and any other tensor that you created yourself)\n",
        "\n",
        "### Q2\n",
        "\n",
        "2a. Implement a two-layer network.\n",
        "\n",
        "2b. Think, what do you think will happen if weights (and bias..) are initialized to zero (using the two layer net)? Now run it and check. Why this has happened?\n",
        "\n",
        "2c. Did the model managed to learn? Why?\n",
        "\n",
        "2d. Train your model end to end on MNIST.\n",
        "\n",
        "### Q3\n",
        "\n",
        "3a. Add a convolution layer to the beginning of the network. (Check Conv2d)\n",
        "(don't forget the non-linearity..)\n",
        "\n",
        "3b. Try to achieve ~1% error on MNIST\n",
        "\n",
        "**Trying other hyperparameters**\n",
        "\n",
        "While you're trying, try different optimizers, different initializations, l2 regularization (try it all after you feel that you can't improve the current model)\n",
        "\n",
        "### Q4\n",
        "\n",
        "Try training a model on CIFAR10. (Try to achieve above 65% accuracy, you can try go for 80% but you'll need to go deeper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFuUPh-WEZQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 0 # <<<<------------ Fill this\n",
        "train_loader, test_loader = mnist_loaders(batch_size, data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQW6Z_2-EZZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement the \"Flatten\" module\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = 0    #  <---------------------- Fill this correctly\n",
        "        return out\n",
        "\n",
        "\n",
        "model = nn.Sequential(\n",
        "    Flatten(),\n",
        "    #nn.Linear(?????, ???????),   # <----------------- Put correct dimensions for linear layer\n",
        ")\n",
        "\n",
        "\n",
        "# next lines check if your model runs correctly\n",
        "X, y = next(iter(test_loader))\n",
        "pred = model(X)\n",
        "err = pred.view(pred.shape[0], -1).max(dim=1)[1].ne(y).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5-NUMfn1JbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "        m.weight.data.zero_()    ## <--------- Note the weights are inititalized to zero\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "            \n",
        "lr = 0 # <----------------- Choose learning rate\n",
        "n_epochs = 0 # <------------- choose number of epochs\n",
        "\n",
        "model.train()\n",
        "# weights initizlization\n",
        "model.apply(weights_init)\n",
        "# loss function\n",
        "lossf = torch.nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "######### this is for statistics (no need to touch)  #########\n",
        "trn_losss = []\n",
        "trn_errs = []\n",
        "val_errs = []\n",
        "plt.ion()\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
        "##############################################################\n",
        "\n",
        "\n",
        "# computing the validation error\n",
        "def compute_error(loader, model):\n",
        "  run_err = AverageValueMeter()\n",
        "  for idx, (samples, targets) in enumerate(train_loader):\n",
        "    outputs = model(samples)\n",
        "    # compute stats\n",
        "    err = outputs.max(1)[1].ne(targets).float().mean().item()\n",
        "    run_err.update(err)\n",
        "    \n",
        "    #break                ##   <-------------- you might consider uncommenting this for debugging\n",
        "  return run_err.avg\n",
        "\n",
        "\n",
        "# iterate on epochs\n",
        "for t in range(n_epochs):\n",
        "  trn_err = AverageValueMeter()\n",
        "  trn_loss = AverageValueMeter()\n",
        "\n",
        "  # iterate on batches\n",
        "  for idx, (samples, targets) in enumerate(train_loader):\n",
        "    \n",
        "    ##################  Change these lines  ####################################\n",
        "    # run the model on the samples and put into the loss function\n",
        "    \n",
        "    \n",
        "    \n",
        "    ##################  Finish changing here  ##################################\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#################### compute stats (no need to change below here) #############\n",
        "    err = outputs.max(1)[1].ne(targets).float().mean().item()\n",
        "    trn_err.update(err)\n",
        "    trn_loss.update(loss.item())    \n",
        "    if idx % 100 == 0:\n",
        "        print('epoch:', t, 'batch:', idx, '/', len(train_loader), 'train-error:',  trn_err.avg)\n",
        "        break\n",
        "   \n",
        "  # computing stats\n",
        "  val_err = compute_error(test_loader, model)\n",
        "  val_errs.append(val_err)    \n",
        "  trn_losss.append(trn_loss.avg)\n",
        "  trn_errs.append(trn_err.avg)\n",
        "  display.clear_output(wait=True)\n",
        "  axes[0].plot(range(len(trn_losss)), trn_losss, 'k')[0]\n",
        "  axes[1].plot(range(len(trn_errs)), trn_errs, 'b')[0]\n",
        "  axes[1].plot(range(len(val_errs)), val_errs, 'r')[0]\n",
        "  display.display(plt.gcf())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSijyBnS1pDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}