{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "romsoc_hands_on_2_handout",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MuKBrPdbB0b",
        "colab_type": "text"
      },
      "source": [
        "# Hands-On Session No. 2\n",
        "## (linear layer implementation and logistic regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZxgJ3YVffj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72dWYuJx1_SV",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOQxYghr4cVR",
        "colab_type": "text"
      },
      "source": [
        "***The batch dimension.*** Usually, the dimensions of tensors are:\n",
        "First dimension is the batch (\"index\" of the sample), then following dimensions are for features. E.g.:\n",
        "\n",
        "N x D: N samples, each with D features (i.e. N vectors)\n",
        "\n",
        "N x H x W: N samples, each a 2D \"image\" of size HxW\n",
        "\n",
        "In the case of images it is very common to have a \"channel\" dimension, e.g.:\n",
        "\n",
        "N x C X H x W: N samples, each a 2D image with C channels. C is most commonly 1 (grayscale image) or 3 (RGB image).\n",
        "\n",
        "(In this exercise will use NxD tensors)\n",
        "\n",
        "***Instructions***\n",
        "\n",
        "1. Define X1, X2 as described below\n",
        "\n",
        "2. Run the cell and see the output figure\n",
        "\n",
        "3. X1, X2 are both sampled from normal distribution. Change their sigmas/mean to have non-overlapping data (since it's ranodm it might sometimes overlap). Try to have the datapoints not too far from each other (to make it a bit more interesting for the classifier).\n",
        "\n",
        "4. Set the labels: 1 for samples in X1, 0 for samples in X2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZlS0LZD1_cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of samples in our trainset. Better to start with a small N\n",
        "# and after you finish the rest of the exercise, come back and try \n",
        "# on larger trainset size\n",
        "N = torch.tensor(5)\n",
        "Nhalf = int(N/2)\n",
        "D = 2\n",
        "\n",
        "# create linearly separable samples using torch.randn\n",
        "# X1: a tensor of dimensions Nhalf x D\n",
        "# X2: a tensor of dimensions (N - Nhalf) x D\n",
        "# both sampled from a standard normal distribution\n",
        "# <<<--- Only change these lines -------->>> \n",
        "X1 = None\n",
        "X2 = None\n",
        "# <<<------------------------------------->>>\n",
        "X = torch.cat([X1, X2])\n",
        "\n",
        "# plot data\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.plot(X[:X1.shape[0], 0].numpy(), X[:X1.shape[0], 1].numpy(), '.r')\n",
        "plt.plot(X[X1.shape[0]:, 0].numpy(), X[X1.shape[0]:, 1].numpy(), '.b')\n",
        "plt.axhline(y=0, color='k')\n",
        "plt.axvline(x=0, color='k')\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "# set labels and save them inside variable y\n",
        "# <<<--- Only change these lines -------->>> \n",
        "y = 0\n",
        "# <<<------------------------------------->>>\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bOOBsUGZx8x",
        "colab_type": "text"
      },
      "source": [
        "## Implement a full forward backward Linear layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpwC0vldZyaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For debugging, put this where you want a breakpoint\n",
        "# import pdb; pdb.set_trace()    \n",
        "# To exit breakpoint write 'exit'\n",
        "\n",
        "class MyLinear(torch.autograd.Function):  \n",
        "  @staticmethod\n",
        "  def forward(ctx, x, w, b):\n",
        "    \"\"\"\n",
        "    @param ctx: a context object, where we can save cache (e.g. tensors) for\n",
        "    later use in the backward part\n",
        "    @param x, w, b: layer's input, weights, bias respectively\n",
        "    forward function returns the output of the layer\n",
        "    \"\"\"\n",
        "    # implement the forward part of the layer, that is the output.\n",
        "    # save in ctx whatever you would need for the backward pass,\n",
        "    # like this: ctx.save_for_backward(a, b, ...) where a, b, ... are tensors\n",
        "    # <<<--- Only change these lines -------->>> \n",
        "\n",
        "    out = None\n",
        "    # ctx.save_for_backward(???)\n",
        "\n",
        "    # <<<------------------------------------>>> \n",
        "\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \"\"\"\n",
        "    @param ctx: context object similar to forward pass (with what you saved)\n",
        "    @param grad_output: a tensor containing the gradient of the loss function\n",
        "    with respect to the layer's output.\n",
        "    \n",
        "    backward function returns the gradient of the loss w.r.t to each of the\n",
        "    layer's inputs. you must compute and return it for each input to forward.\n",
        "    \"\"\"\n",
        "    # implement the gradients of the loss per each input to forward: dx, dw, db\n",
        "    # you can read what you saved earlier in ctx like this:\n",
        "    # a, b, ... = ctx.saved_tensors\n",
        "    # <<<--- Only change these lines -------->>> \n",
        "\n",
        "    # ???, ???, ... = ctx.saved_tensors\n",
        "    dx = None\n",
        "    dw = None\n",
        "    db = None\n",
        "    # <<<------------------------------------>>> \n",
        "\n",
        "    return dx, dw, db\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2hqW9ljPUvq",
        "colab_type": "text"
      },
      "source": [
        "## Check that the above code works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrIbjEOGdrmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of outputs. For logistic regression we will use one output.\n",
        "K = 1\n",
        "\n",
        "# initialize parameters\n",
        "# w: a tensor of dimensions D x K\n",
        "# b: a tensor of dimensions K\n",
        "# make sure they have requires_grad=True\n",
        "# <<<--- Only change these lines -------->>> \n",
        "w = None\n",
        "b = None\n",
        "# <<<------------------------------------>>> \n",
        "\n",
        "# initialize the layer and run forward and backward passes\n",
        "lin = MyLinear()\n",
        "out = lin.apply(X, w, b).sigmoid()\n",
        "loss = torch.nn.BCELoss()(out, y.float().unsqueeze(-1))\n",
        "# run the backward pass (this will compute the gradients)\n",
        "loss.backward()\n",
        "print(loss)\n",
        "\n",
        "# you can enter the gradient of each layer like this:\n",
        "print(w.grad)\n",
        "# if the line above throws an error it probably means you didn't\n",
        "# define w to have requires_grad = True\n",
        "\n",
        "\n",
        "## Check gradients\n",
        "## Once you think you're done, uncomment the following lines\n",
        "## to make sure the gradients you computed are correct\n",
        "\n",
        "# from torch.autograd import gradcheck\n",
        "# X_check = X.clone().double().requires_grad_(True)\n",
        "# w_check = w.clone().double().requires_grad_(True)\n",
        "# b_check = b.clone().double().requires_grad_(True)\n",
        "# f = lin.apply\n",
        "# gradcheck(f, (X_check,w_check,b_check))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJryQ7f5R5-f",
        "colab_type": "text"
      },
      "source": [
        "## Train a linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHR35zXxR5Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General Parameters\n",
        "nepochs = 5\n",
        "lr = 0\n",
        "K = 1\n",
        "print_every = 1\n",
        "#####################\n",
        "\n",
        "# Initialize paraneters\n",
        "torch.manual_seed(0)\n",
        "# copy here the w,b that you defined earlier\n",
        "# <<<--- Only change these lines -------->>> \n",
        "w = None\n",
        "b = None\n",
        "# <<<------------------------------------>>> \n",
        "\n",
        "\n",
        "def zero_grad(w):\n",
        "  if hasattr(w, 'grad'):\n",
        "    if w.grad is not None:\n",
        "      w.grad.zero_()\n",
        "  \n",
        "lin = MyLinear()\n",
        "\n",
        "# Train model\n",
        "for t in range(nepochs):\n",
        "  out = lin.apply(X, w, b).sigmoid()\n",
        "  loss = torch.nn.BCELoss()(out, y.float().unsqueeze(-1))\n",
        "\n",
        "  if t % print_every == 0:\n",
        "    print(t, 'loss:', loss.item())\n",
        "  \n",
        "  # compute gradients to parameters\n",
        "  # make a backward pass through \"loss\"\n",
        "  # don't forget that with pytorch tensors, gradients are summed with\n",
        "  # what is already there...\n",
        "  # <<<--- Only change these lines -------->>> \n",
        "\n",
        " \n",
        "  # <<<------------------------------------>>> \n",
        "  \n",
        "  # Update parameters using the simple gradient descent method\n",
        "  # <<<--- Only change these lines -------->>> \n",
        "  w.data = 0\n",
        "  b.data = 0\n",
        "  # <<<------------------------------------>>> \n",
        "\n",
        "print(t, 'loss:', loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzaUbch0uuEA",
        "colab_type": "text"
      },
      "source": [
        "## Verify correctness of solution\n",
        "\n",
        "When you think you're finished trainig the model, run the following code to see if your final parameters that you found make sense.\n",
        "\n",
        "The green line is the separating hyperplane that you've found"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwDoMenjjiP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot data\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.plot(X[:X1.shape[0], 0].numpy(), X[:X1.shape[0], 1].numpy(), '.r')\n",
        "plt.plot(X[X1.shape[0]:, 0].numpy(), X[X1.shape[0]:, 1].numpy(), '.b')\n",
        "plt.axhline(y=0, color='k')\n",
        "plt.axvline(x=0, color='k')\n",
        "\n",
        "xmin = X[:, 0].min()\n",
        "xmax = X[:, 0].max()\n",
        "ymin = (-b - w[0] * xmin) / w[1]\n",
        "ymax = (-b - w[0] * xmax) / w[1]\n",
        "plt.plot([xmin, xmax], [ymin, ymax], 'g')\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHkVsuFDPFuD",
        "colab_type": "text"
      },
      "source": [
        "## Now use pytorch.nn do it for you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91WGx4PUAoMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General Parameters\n",
        "nepochs = 5\n",
        "lr = 0\n",
        "K = 1\n",
        "print_every = 1\n",
        "#####################\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Use torch.nn Linear layer to define a linear layer\n",
        "# similar to the one we used above\n",
        "# put it inside \"lin\" variable\n",
        "# <<<--- Only change these lines -------->>> \n",
        "lin = None\n",
        "\n",
        "# <<<------------------------------------>>> \n",
        "\n",
        "\n",
        "# you would have to define the optimizer that you will use\n",
        "# use a gradient descent optimizer from torch.optim\n",
        "# <<<--- Only change these lines -------->>> \n",
        "optimizer = None\n",
        "\n",
        "# <<<------------------------------------>>> \n",
        "\n",
        "  \n",
        "# Train model\n",
        "for t in range(nepochs):\n",
        "  \n",
        "  # use \"lin\" from above to compute the output\n",
        "  # of the logistic regression. don't forget the sigmoid..\n",
        "  # <<<--- Only change these lines -------->>> \n",
        "  out = None\n",
        "\n",
        "  # <<<------------------------------------>>> \n",
        "\n",
        "  loss = torch.nn.BCELoss()(out, y.float().unsqueeze(-1))\n",
        "  if t % print_every == 0:\n",
        "    print(t, 'loss:', loss.item())\n",
        "  \n",
        "  # compute gradients to parameters\n",
        "  # call the backward pass from loss.\n",
        "  # note that we didn't explicitly write any backward function.\n",
        "  # that's one of the biggest advantages of using modern DL framework.\n",
        "  # (remember that gradients are being summed...)\n",
        "  # <<<--- Only change these lines -------->>> \n",
        "\n",
        "\n",
        "  # <<<------------------------------------>>> \n",
        "\n",
        "\n",
        "  # update parameters\n",
        "  # use the optimizer to update the parameters\n",
        "  # <<<--- Only change these lines -------->>> \n",
        "\n",
        "\n",
        "  # <<<------------------------------------>>> \n",
        "\n",
        "  \n",
        "print(t, 'loss:', loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgIh6ivMLffi",
        "colab_type": "text"
      },
      "source": [
        "## More Questions\n",
        "\n",
        "1. Did you manage to converge to zero loss? If not, why?\n",
        "2. Change data points to overlap (change sigmas and/or means). Try to converge. Did you converge to zero loss? why did you manage (or didn't)?"
      ]
    }
  ]
}